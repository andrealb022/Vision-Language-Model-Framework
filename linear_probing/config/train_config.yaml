# --- Modello ---
model:
  name: llava           # es: llava, qwen, blip2...
  quantization: fp32    # 4bit | 8bit | fp16 | fp32
  freeze_backbone: true  # freeze backbone
  # --- Testa ---
  dropout_p: 0.3        # dropout nella testa
  deeper_head: true     # true = head pi√π profonda
  hidden_dim: 512        # dimensione layer nascosto se deeper_head = true

# --- Task di classificazione ---
task: age           # gender | ethnicity | emotion | age

# --- Dati & Loader ---
data:
  base_path: null       # oppure "/path/to/dataset"
  dataset_mode: auto    # "auto" = sceglie in base al task, "filter" solo i dataset in dataset_filter
  dataset_filter: []    # opzionale: ["FairFace","RAF-DB"] per usare solo questi (se non vuoto, ignora auto)
  batch_size: 128
  num_workers: 8
  val_split: 0.2        # frazione validation

# --- Training ---
train:
  epochs: 50
  lr: 0.001
  weight_decay: 0.0001
  patience: 5
  seed: 42
  amp: true             # mixed precision se CUDA presente
  scheduler:
    type: cosine_wr     # per ora: cosine_wr
    T_0: 10
    T_mult: 2

# --- Ripresa ---
resume:
  from: null            # es: "/.../checkpoints/llava_fp32_auto_age_head_linear"